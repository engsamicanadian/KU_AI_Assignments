{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIOFUMTrFnds",
        "outputId": "38efd7eb-5a7c-4bc0-edbe-57ebf4267ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5)]\n",
            "(king (13), husbands (2254)) -> 0\n",
            "(king (13), james (1154)) -> 1\n",
            "(james (1154), bible (5766)) -> 1\n",
            "(james (1154), king (13)) -> 1\n",
            "(james (1154), thyatira (5733)) -> 0\n",
            "(bible (5766), venomous (11860)) -> 0\n",
            "(bible (5766), spendeth (6448)) -> 0\n",
            "(james (1154), micah (1612)) -> 0\n",
            "(bible (5766), james (1154)) -> 1\n",
            "(king (13), hodesh (9832)) -> 0\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " embedding_4_input (InputLayer)  [(None, 1)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_5_input (InputLayer)  [(None, 1)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 1, 100)       1242500     ['embedding_4_input[0][0]']      \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 1, 100)       1242500     ['embedding_5_input[0][0]']      \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)            (None, 100)          0           ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)            (None, 100)          0           ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 100)          0           ['reshape_4[0][0]',              \n",
            "                                                                  'reshape_5[0][0]']              \n",
            "                                                                                                  \n",
            " sequential_8 (Sequential)      (None, 1)            101         ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,485,101\n",
            "Trainable params: 2,485,101\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 3635.0100488313474\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 3180.4145999003667\n",
            "(12424, 12424)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'egypt': ['send', 'feast', 'get', 'heave', 'sabbath'],\n",
              " 'famine': ['wroth', 'hur', 'earrings', 'horns', 'breaking'],\n",
              " 'god': ['land', 'house', 'congregation', 'kept', 'heart'],\n",
              " 'jesus': ['shortly', 'searcheth', 'pleasing', 'appearing', 'proof'],\n",
              " 'john': ['principalities', 'leaveth', 'disputings', 'leadeth', 'tempting']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "from nltk.corpus import gutenberg # to get bible corpus\n",
        "from string import punctuation # to remove punctuation from corpus\n",
        "import nltk \n",
        "import numpy as np\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing.sequence import skipgrams \n",
        "from keras.layers import *\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model,Sequential\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "bible = gutenberg.sents(\"bible-kjv.txt\")\n",
        "remove_terms = punctuation + '0123456789'\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "def normalize_document(doc):\n",
        "     # lower case and remove special characters\\whitespaces\n",
        "     doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A)\n",
        "     doc = doc.lower()\n",
        "     doc = doc.strip()\n",
        "     # tokenize document\n",
        "     tokens = wpt.tokenize(doc)\n",
        "     # filter stopwords out of document\n",
        "     filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "     # re-create document from filtered tokens\n",
        "     doc = ' '.join(filtered_tokens)\n",
        "     return doc\n",
        "normalize_corpus = np.vectorize(normalize_document) \n",
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "vocab_size = len(word2id) + 1\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:5]) \n",
        "# generate skip-grams\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "     print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "           id2word[pairs[i][0]], pairs[i][0], \n",
        "           id2word[pairs[i][1]], pairs[i][1], \n",
        "           labels[i]))\n",
        "\n",
        "# build skip-gram architecture\n",
        "embed_size = 100\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                          embeddings_initializer=\"glorot_uniform\",\n",
        "                          input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                   embeddings_initializer=\"glorot_uniform\",\n",
        "                   input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "merged_output = add([word_model.output, context_model.output])  \n",
        "model_combined = Sequential()\n",
        "model_combined.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "final_model = Model([word_model.input, context_model.input], model_combined(merged_output))\n",
        "final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "final_model.summary()\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(final_model, show_shapes=True, show_layer_names=False, \n",
        "                  rankdir='TB').create(prog='dot', format='svg')) \n",
        "for epoch in range(1, 3):\n",
        "     loss = 0\n",
        "     for i, elem in enumerate(skip_grams):\n",
        "         pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "         pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "         labels = np.array(elem[1], dtype='int32')\n",
        "         X = [pair_first_elem, pair_second_elem]\n",
        "         Y = labels\n",
        "         if i % 10000 == 0:\n",
        "             print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "         loss += final_model.train_on_batch(X,Y)  \n",
        "     print('Epoch:', epoch, 'Loss:', loss) \n",
        "\n",
        "word_embed_layer = word_model.layers[0]\n",
        "weights = word_embed_layer.get_weights()[0][1:]\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                    for search_term in ['god', 'jesus','egypt', 'john', 'famine']}\n",
        "similar_words \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}